{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable, NamedTuple\n",
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de tipos\n",
    "FuncionActivacion = Tuple[Callable[[np.ndarray], np.ndarray],\n",
    "    Callable[[np.ndarray], np.ndarray]]\n",
    "Pesos = np.ndarray\n",
    "CapaNeuronal = Tuple[Pesos, np.ndarray, FuncionActivacion]\n",
    "Modelo = NamedTuple('Modelo',\n",
    "    [(\"entrenar\",\n",
    "        Callable[[List[List[int]], List[int], int], List[CapaNeuronal]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de activación y sus derivadas\n",
    "def sigmoide(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def derivada_sigmoide(valor_activacion: np.ndarray) -> np.ndarray:\n",
    "    return valor_activacion * (1 - valor_activacion)\n",
    "\n",
    "def derivada_tanh(valor_activacion: np.ndarray) -> np.ndarray:\n",
    "    return 1 - valor_activacion**2\n",
    "\n",
    "def derivada_relu(valor_activacion: np.ndarray) -> np.ndarray:\n",
    "    return (valor_activacion > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_capas_neuronales(neuronas_por_capa: List[int],\n",
    "        funciones_activacion: List[FuncionActivacion]) -> List[CapaNeuronal]:\n",
    "    \"\"\"\n",
    "    Crear una lista de capas neuronales con pesos, sesgos y funciones de\n",
    "    activación.\n",
    "\n",
    "    :param neuronas_por_capa: Lista con el número de neuronas en cada capa.\n",
    "    :param funciones_activacion: Lista con funciones de activación para cada\n",
    "    capa.\n",
    "    :return: Lista de capas neuronales, cada una de las cuales es una tupla con\n",
    "    pesos, sesgos y función de activación.\n",
    "    \"\"\"\n",
    "    capas_neuronales = []\n",
    "    for capa_actual, siguiente_capa, funcion_activacion \\\n",
    "            in zip(neuronas_por_capa[:-1], neuronas_por_capa[1:],\n",
    "                funciones_activacion):\n",
    "        pesos = np.random.randn(siguiente_capa, capa_actual) * \\\n",
    "            np.sqrt(2. / capa_actual) # Método de He\n",
    "        sesgos = np.zeros((siguiente_capa, 1))\n",
    "        capas_neuronales.append((pesos, sesgos, funcion_activacion))\n",
    "    return capas_neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagacion_hacia_adelante(entradas: np.ndarray,\n",
    "        lista_capas_neuronales: List[CapaNeuronal]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Realizar la propagación hacia adelante a través de la red neuronal.\n",
    "\n",
    "    :param entradas: Matriz de entradas donde cada columna es un caso y cada\n",
    "    fila un nodo de la capa actual.\n",
    "    :param lista_capas_neuronales: Lista de capas neuronales.\n",
    "    :return: Lista de valores de activación para cada capa\n",
    "    \"\"\"\n",
    "    valores_activacion_capa = [entradas]\n",
    "    # iteración secuencial\n",
    "    for pesos, sesgos, funcion_activacion in lista_capas_neuronales:\n",
    "        z = np.dot(pesos, valores_activacion_capa[-1]) + sesgos\n",
    "        valor_activacion = funcion_activacion[0](z)\n",
    "        valores_activacion_capa.append(valor_activacion)\n",
    "    return valores_activacion_capa[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_cuadratico(predicho: np.ndarray, deseado: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcula el error cuadrático entre los valores predichos y los valores\n",
    "    deseados.\n",
    "\n",
    "    :param predicho: Valores predichos por el modelo.\n",
    "    :param deseado: Valores deseados.\n",
    "    :return: Error cuadrático.\n",
    "    \"\"\"\n",
    "    return np.mean((deseado - predicho)**2)\n",
    "\n",
    "def error_cuadratico_derivada(predicho: np.ndarray,\n",
    "        deseado: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcular la derivada del error cuadrático entre los valores predichos y los\n",
    "    valores deseados.\n",
    "\n",
    "    :param predicho: Valores predichos por el modelo.\n",
    "    :param deseado: Valores deseados.\n",
    "    :return: Derivada del error cuadrático\n",
    "    \"\"\"\n",
    "    return 2 * (deseado - predicho) / predicho.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retropropagacion(entradas: np.ndarray, salidas: np.ndarray,\n",
    "        resultados_capa_z: List[np.ndarray],\n",
    "        lista_capas_neuronales: List[CapaNeuronal]) \\\n",
    "            -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Realizar la retropropagación a través de la red neuronal para calcular los\n",
    "    gradientes.\n",
    "\n",
    "    :param entradas: Entradas de la red neuronal.\n",
    "    :param salidas: Valores deseados de salida.\n",
    "    :param resultados_capa_z: Resultados de la activación para cada capa.\n",
    "    :param lista_capas_neuronales: Lista de capas neuronales.\n",
    "    :return: Lista de gradientes parciales (dZ, dW, db) para cada capa.\n",
    "    \"\"\"\n",
    "\n",
    "    gradientes_parciales = []\n",
    "    \n",
    "    # Última capa\n",
    "    pesos_ultima_capa, _, funciones_activacion_ultima_capa \\\n",
    "        = lista_capas_neuronales[-1]\n",
    "    valor_final = resultados_capa_z[-1]\n",
    "    derivada_costo = error_cuadratico_derivada(valor_final, salidas)\n",
    "    dA = derivada_costo * funciones_activacion_ultima_capa[1](valor_final)\n",
    "    dW = np.dot(dA, resultados_capa_z[-2].T)\n",
    "    db = np.sum(dA, axis=1, keepdims=True)\n",
    "    dZ = np.dot(pesos_ultima_capa.T, dA)\n",
    "\n",
    "    gradientes_parciales.append((dZ, dW, db))\n",
    "\n",
    "    # Retropropagación para las capas anteriores\n",
    "    for i in range(len(lista_capas_neuronales) - 2, -1, -1):\n",
    "        pesos, _, funcion_activacion = lista_capas_neuronales[i]\n",
    "        dZ = np.multiply(np.dot(pesos.T, dZ),\n",
    "            funcion_activacion[1](resultados_capa_z[i]))\n",
    "        dW = np.dot(dZ, resultados_capa_z[i - 1].T)\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)\n",
    "        gradientes_parciales.append((dZ, dW, db))\n",
    "    \n",
    "    # Invertir la lista para que esté en el orden correcto\n",
    "    return gradientes_parciales[::1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_parametros(lista_capas_neuronales: List[CapaNeuronal],\n",
    "        gradientes: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],\n",
    "        tasa_aprendizaje: float) -> List[CapaNeuronal]:\n",
    "    \"\"\"\n",
    "    Actualizar los parámetros (pesos y sesgos) de las capas neuronales usando\n",
    "    gradientes y tasa de aprendizaje.\n",
    "\n",
    "    :param gradientes: Lista de gradientes (dZ, dW, db) para cada capa.\n",
    "    :param tasa_aprendizaje: Tasa de aprendizaje para la actualización.\n",
    "    :return: Lista actualizada de capas neuronales.\n",
    "    \"\"\"\n",
    "    nueva_lista_capas_neuronales = []\n",
    "    for (pesos, sesgos, funcion_activacion), (_, dW, db) \\\n",
    "            in zip(lista_capas_neuronales, gradientes):\n",
    "        nuevos_pesos = pesos - tasa_aprendizaje * dW\n",
    "        nuevos_sesgos = sesgos - tasa_aprendizaje * db\n",
    "        nueva_lista_capas_neuronales.append((nuevos_pesos, nuevos_sesgos,\n",
    "            funcion_activacion))\n",
    "    return nueva_lista_capas_neuronales\n",
    "\n",
    "def entrenar(lista_capas_neuronales_inicial: List[CapaNeuronal],\n",
    "        tasa_aprendizaje: float, entradas: List[List[int]], salidas: List[int],\n",
    "        epocas: int = 1000) -> List[CapaNeuronal]:\n",
    "    \"\"\"\n",
    "    Entrenar la red neuronal utilizando el algoritmo de retropropagación.\n",
    "\n",
    "    :param lista_capas_neuronales_inicial: Lista inicial de capas neuronales.\n",
    "    :param tasa_aprendizaje: Tasa de aprendizaje para la actualización de\n",
    "    parámetros.\n",
    "    :param entradas: Datos de entrada para el entrenamiento.\n",
    "    :param salidas: Valores deseados de salida para el entrenamiento.\n",
    "    :param epocas: Número de épocas para entrenar la red.\n",
    "    :return: Lista de capas neuronales entrenadas.\n",
    "    \"\"\"\n",
    "    lista_capas_neuronales = lista_capas_neuronales_inicial\n",
    "    vector_entradas = np.array(entradas).T\n",
    "    vector_salidas = np.array(salidas).T\n",
    "\n",
    "    for epoca in range(1, epocas + 1):\n",
    "        resultados_capas = propagacion_hacia_adelante(vector_entradas,\n",
    "            lista_capas_neuronales)\n",
    "        costo = error_cuadratico(resultados_capas[-1],\n",
    "            vector_salidas) # Error promedio\n",
    "        gradientes = retropropagacion(vector_entradas, vector_salidas,\n",
    "            resultados_capas, lista_capas_neuronales)\n",
    "        lista_capas_neuronales = actualizar_parametros(lista_capas_neuronales,\n",
    "            gradientes, tasa_aprendizaje)\n",
    "\n",
    "        if epoca % 100 == 0:\n",
    "            print(f\"Costo después de la iteración #{epoca}: {costo}\")\n",
    "    \n",
    "    return lista_capas_neuronales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_modelo_red_neuronal(neuronas_por_capa: List[int],\n",
    "        funciones_activacion: List[FuncionActivacion],\n",
    "        tasa_aprendizaje: float) -> Modelo:\n",
    "    \"\"\"\n",
    "    Crear un modelo de red neuronal con las características dadas.\n",
    "\n",
    "    :param neuronas_por_capa: Lista con el número de neuronas en cada capa.\n",
    "    :param funciones_activacion: Lista de funciones de activación por cada\n",
    "    capa.\n",
    "    :param tasa_aprendizaje: Tasa de aprendizaje para el entrenamiento.\n",
    "    :return: Modelo de red neuronal.\n",
    "    \"\"\"\n",
    "    lista_capas_neuronales = crear_capas_neuronales(neuronas_por_capa,\n",
    "        funciones_activacion)\n",
    "    print(f\"Modelo inicial aleatorio: {lista_capas_neuronales}\")\n",
    "    return Modelo(partial(entrenar, lista_capas_neuronales, tasa_aprendizaje))\n",
    "\n",
    "def predecir(entradas: np.ndarray,\n",
    "        lista_capas_neuronales_entrenadas: List[CapaNeuronal]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Realizar predicciones usando la red neuronal entrenada.\n",
    "\n",
    "    :param entradas: Datos de entrada para la predicción.\n",
    "    :param lista_capas_neuronales_entrenadas: Lista de capas neuronales\n",
    "    entrenadas.\n",
    "    :return: Resultados de la predicción. Los valores son verdaderos si son\n",
    "    mayores que 0.5, de lo contrario son falsos.\n",
    "    \"\"\"\n",
    "    resultados_capas = propagacion_hacia_adelante(entradas,\n",
    "        lista_capas_neuronales_entrenadas)\n",
    "    predicciones = resultados_capas[-1]\n",
    "    predicciones = np.squeeze(predicciones)\n",
    "    return np.greater(predicciones, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo inicial aleatorio: [(array([[ 1.62434536, -0.61175641],\n",
      "       [-0.52817175, -1.07296862]]), array([[0.],\n",
      "       [0.]]), (<ufunc 'tanh'>, <function derivada_tanh at 0x7f78a9fda3e0>)), (array([[ 0.86540763, -2.3015387 ],\n",
      "       [ 1.74481176, -0.7612069 ]]), array([[0.],\n",
      "       [0.]]), (<function sigmoide at 0x7f78a9f17ce0>, <function derivada_sigmoide at 0x7f78a9fdaac0>))]\n",
      "Costo después de la iteración #100: 0.38100631595546075\n",
      "Costo después de la iteración #200: 0.3738643837759322\n",
      "Costo después de la iteración #300: 0.37293639557532804\n",
      "Costo después de la iteración #400: 0.37276249145046053\n",
      "Costo después de la iteración #500: 0.37273256482474526\n",
      "Costo después de la iteración #600: 0.37274192800893735\n",
      "Costo después de la iteración #700: 0.3727666422714443\n",
      "Costo después de la iteración #800: 0.37280060726170555\n",
      "Costo después de la iteración #900: 0.37284239546178155\n",
      "Costo después de la iteración #1000: 0.37289173805748066\n",
      "La predicción de la red neuronal para el ejemplar[[1 1 0 0]\n",
      " [1 0 1 0]] es [[ True  True  True  True]\n",
      " [ True  True  True False]]\n",
      "Resultados de la propagación hacia adelante en X_prueba: [array([[0.99990575, 0.99976455, 0.97209098, 0.93169886],\n",
      "       [1.        , 0.99999976, 0.99999847, 0.99957625]]), array([[0.99917918, 0.99917876, 0.99909264, 0.99895035],\n",
      "       [0.51212567, 0.51207787, 0.50269381, 0.4891029 ]])]\n"
     ]
    }
   ],
   "source": [
    "# Semilla para reproducir los resultados\n",
    "np.random.seed(1)\n",
    "\n",
    "# Datos de entrenamiento (4 ejemplos por columna)\n",
    "entradas = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "# Salidas deseadas para cada ejemplo en X\n",
    "salidas = [0, 1, 1, 0]\n",
    "\n",
    "# Datos de prueba (vector de 2x1 para calcular el XOR)\n",
    "# Prueba con (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "X_prueba = np.array([[1, 1, 0, 0], [1, 0, 1, 0]])\n",
    "\n",
    "# Parámetros del modelo\n",
    "neuronas_por_capa = [2, 2, 2, 1]\n",
    "funciones_activacion = [\n",
    "    (np.tanh, derivada_tanh),\n",
    "    (sigmoide, derivada_sigmoide),\n",
    "]\n",
    "num_iteraciones = 1000\n",
    "tasa_aprendizaje = 0.3\n",
    "\n",
    "# Crear y entrenar el modelo de red neuronal\n",
    "modelo_rn = crear_modelo_red_neuronal(neuronas_por_capa,\n",
    "    funciones_activacion,tasa_aprendizaje)\n",
    "modelo_entrenado = modelo_rn.entrenar(entradas, salidas, num_iteraciones)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_prediccion = predecir(X_prueba, modelo_entrenado)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"La predicción de la red neuronal para el ejemplar\" +\n",
    "    f\"{X_prueba} es {y_prediccion}\")\n",
    "print(\"Resultados de la propagación hacia adelante en X_prueba: \" +\n",
    "    f\"{propagacion_hacia_adelante(X_prueba, modelo_entrenado)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
